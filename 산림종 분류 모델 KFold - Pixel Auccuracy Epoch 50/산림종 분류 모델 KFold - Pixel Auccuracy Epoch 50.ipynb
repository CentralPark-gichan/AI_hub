{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version은 1.4으로 맞춰서 실행해야합니다. \n",
    "!conda install -y pytorch torchvision torchaudio cudatoolkit=10.1 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch version은 1.4으로 맞춰서 실행해야합니다. \n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, phase='train', img_cropped = None, label_cropped = None):\n",
    "        self.root = root\n",
    "        self.phase = phase\n",
    "        self.data = {}\n",
    "                    \n",
    "        # 마찬가지로 TEST 데이터의 경우에도 이미지를 늘리고 다시 줄인 듯\n",
    "        # 해당 부분에 대해서 TRAIN과 마찬가지로 CROP해서 결합해도 괜찮을 듯 \n",
    "        if phase == 'test':\n",
    "            img_cropped = []\n",
    "            for i in range(238):\n",
    "                img = cv2.imread(self.root + f'/test/{i}.png')\n",
    "                img_cropped.append(img)\n",
    "        \n",
    "        self.data['input'] = img_cropped\n",
    "        self.data['output'] = label_cropped            \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 실험해야할 부분 \n",
    "        # Augmentation으로 수정해야함 \n",
    "        transform_tr = transforms.Compose([\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "                ])\n",
    "\n",
    "        transform_te = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "                ])       \n",
    "        \n",
    "        img = self.data['input'][index]        \n",
    "        input_img = transform_tr(img)\n",
    "                \n",
    "        if (self.phase == 'train') | (self.phase == 'valid'):\n",
    "            mask = self.data['output'][index]\n",
    "            output_mask = transform_te(mask)\n",
    "            return (input_img, output_mask)\n",
    "        else:\n",
    "            dummy = []\n",
    "            return (input_img, dummy)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['input'])\n",
    "    \n",
    "    def get_label_file(self):\n",
    "        return self.label_path\n",
    "\n",
    "def data_loader(root, phase='train', batch_size=4, img_cropped = None, label_cropped = None):\n",
    "    if phase == 'train':\n",
    "        shuffle = True\n",
    "    else:\n",
    "        shuffle = False\n",
    "    \n",
    "    dataset = CustomDataset(root, phase, img_cropped = img_cropped, label_cropped = label_cropped)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse\n",
    "\n",
    "DATASET_PATH = '/home/workspace/data/.train/.task142/'\n",
    "\n",
    "\n",
    "def _infer(model, cuda, data_loader):\n",
    "    pred = []\n",
    "    for idx, (img, _) in enumerate(data_loader):\n",
    "        if cuda:\n",
    "            img = img.cuda()\n",
    "\n",
    "        output = model(img)\n",
    "        output = output['out'].detach().cpu().numpy()\n",
    "        output = output.argmax(1)\n",
    "        pred.extend(output)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def feed_infer(output_file, infer_func, fold):\n",
    "    prediction = infer_func()\n",
    "    print('write output')\n",
    "    out_pred = pd.DataFrame()\n",
    "    for pred in prediction:\n",
    "        pred = pd.DataFrame(pred)\n",
    "        out_pred = pd.concat([out_pred, pred], axis=0)\n",
    "    out_pred = out_pred + 1\n",
    "    out_pred.to_csv('/home/workspace/user-workspace/prediction/prediction{}.csv'.format(fold), index=None)\n",
    "    \n",
    "    if os.stat(output_file).st_size == 0:\n",
    "        raise AssertionError('output result of inference is nothing')\n",
    "\n",
    "\n",
    "def test(prediction_file_name, model, test_dataloader, cuda, fold):\n",
    "    feed_infer(prediction_file, lambda : _infer(model, cuda, data_loader=test_dataloader), fold=fold)\n",
    "\n",
    "\n",
    "def save_model(model_name, model, optimizer, scheduler, fold):\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, 'deep_resnet101_pixel50_' + fold + '.pth')\n",
    "    print('model saved')\n",
    "\n",
    "\n",
    "def load_model(model_name, model, optimizer=None, scheduler=None):\n",
    "    state = torch.load(os.path.join(model_name))\n",
    "    model.load_state_dict(state['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "    print('model loaded')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cropped = []\n",
    "label_cropped = []\n",
    "\n",
    "root = '/home/workspace/data/.train/.task142/'\n",
    "\n",
    "data_path=os.path.join(root, 'train.png')\n",
    "label_path=os.path.join(root, 'train_label.png')\n",
    "img = cv2.imread(data_path)\n",
    "\n",
    "# class_mask가 1부터 시작하므로, 0부터 시작하도록 세팅합니다. \n",
    "class_mask = np.array(pd.read_csv(root + 'class_mask.csv'), dtype=np.int) - 1\n",
    "class_mask = to_categorical(class_mask, 9)\n",
    "\n",
    "for i in tqdm(range(0, 8750-512, 512)):\n",
    "    for j in range(0, 7225-512, 512):\n",
    "        img_cropped.append(img[i:i+512, j:j+512].copy())\n",
    "        label_cropped.append(class_mask[i:i+512, j:j+512].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation의 정확한 점수를 체크하기 위해서 pixel accuracy 계산 \n",
    "def pixel_accuracy(pred, sol):\n",
    "    pred = pred.argmax(axis=1)\n",
    "    sol = sol.argmax(axis=1)\n",
    "    \n",
    "    acc_sum = np.sum((pred == sol))\n",
    "    acc = float(acc_sum / (512 * 512 + 1e-10))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드의 고정을 위해서 seed 세팅 \n",
    "import random\n",
    "import gc\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 고정 \n",
    "base_lr = 0.003\n",
    "cuda = True\n",
    "num_epochs = 50\n",
    "print_iter = 50\n",
    "model_name = \"deep_resnet101\"\n",
    "prediction_file = \"prediction.csv\"\n",
    "batch = 8\n",
    "mode = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5폴드 분할 후 학습 진행 \n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "kf = KFold(5, shuffle=True, random_state=0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "# 데이터를 Numpy 형태로 변환 \n",
    "img_cropped = np.array(img_cropped)\n",
    "label_cropped = np.array(label_cropped)\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(range(0, len(img_cropped)))):        \n",
    "    print(\"#\"*50, fold, \"#\"*50)\n",
    "    # 데이터셋 로드 \n",
    "    train_dataloader = data_loader(root=DATASET_PATH, phase='train', batch_size=batch, img_cropped = img_cropped[tr_idx], label_cropped = label_cropped[tr_idx])\n",
    "    valid_dataloader = data_loader(root=DATASET_PATH, phase='valid', batch_size=batch, img_cropped = img_cropped[val_idx], label_cropped = label_cropped[val_idx])\n",
    "\n",
    "    time_ = datetime.datetime.now()\n",
    "    num_batches = len(train_dataloader)\n",
    "    best_valid = 0\n",
    "    # Deeplabv3 모델 불러오기 \n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=9).cuda()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    # 로스 함수 설정 \n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "    loss_fn = loss_fn.cuda()\n",
    "    \n",
    "    # optimizer, scheduler 함수 설정 \n",
    "    optimizer = Adam([param for param in model.parameters() if param.requires_grad],lr=base_lr, weight_decay=1e-4)\n",
    "    scheduler = StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "    \n",
    "    # 50번의 Epoch를 반복 \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, valid_loss, train_pixel ,valid_pixel  = 0, 0, 0, 0\n",
    "        model.train()\n",
    "        for iter_, (i, l) in enumerate(train_dataloader):\n",
    "            i = i.cuda()\n",
    "            l = l.cuda()\n",
    "            out = model(i)\n",
    "            loss = loss_fn(out['out'].float(), l.float())\n",
    "            train_pixel += pixel_accuracy(out['out'].float().cpu().detach().numpy(), l.float().cpu().detach().numpy()) / len(train_dataloader.dataset.data['input'])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() / (512*512*9*len(train_dataloader.dataset.data['input']))\n",
    "            del loss; del i; del l; del out; gc.collect()\n",
    "            \n",
    "        model.eval()\n",
    "        for iter_, (i, l) in enumerate(valid_dataloader):\n",
    "            i = i.cuda()\n",
    "            l = l.cuda()\n",
    "            out = model(i)\n",
    "            loss = loss_fn(out['out'].float(), l.float())\n",
    "            valid_pixel += pixel_accuracy(out['out'].float().cpu().detach().numpy(), l.float().cpu().detach().numpy()) / len(valid_dataloader.dataset.data['input'])\n",
    "            valid_loss += loss.item() / (512*512*9*len(valid_dataloader.dataset.data['input']))\n",
    "            del loss; del i; del l; del out; gc.collect()\n",
    "            \n",
    "        \n",
    "        if (iter_ + 1) % 1 == 0:\n",
    "            elapsed = datetime.datetime.now() - time_\n",
    "            print('[{:.0f}/{:d}] train loss({:.4f}) valid loss({:.4f}) train pixel Accuracy({:.4f}) valid pixel Accuracy({:.4f})'.format(\n",
    "                      int(epoch), num_epochs, train_loss, valid_loss, train_pixel, valid_pixel))\n",
    "            time_ = datetime.datetime.now()\n",
    "\n",
    "        # scheduler update\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 가장 좋은 모델을 Valid Pixel Accuracy를 기준으로 선택 \n",
    "        if best_valid < valid_pixel: \n",
    "            best_valid = valid_pixel\n",
    "            # save model\n",
    "            save_model(str(epoch + 1), model, optimizer, scheduler, str(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습과정과 추론과정을 분리하기 위해서, 학습된 파라미터를 불러와서 실행 \n",
    "## 참고로 여기에서는 Torch 1.6버전을 사용해도 무방했습니다. \n",
    "import gc\n",
    "mode = \"test\"\n",
    "root = '/home/workspace/data/.train/.task142/'\n",
    "\n",
    "batch = 1\n",
    "\n",
    "preds = []\n",
    "for i in range(5):\n",
    "    # 저장된 모델의 가중치를 불러와서 로드\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=9).cuda()\n",
    "    model = load_model('deep_resnet101_pixel50_{}.pth'.format(i), model)\n",
    "    \n",
    "    model.eval()\n",
    "    # get data loader\n",
    "    test_dataloader = data_loader(root=DATASET_PATH, phase=mode, batch_size=batch)\n",
    "    pred = []\n",
    "    # 데이터를 불러와서 예측 \n",
    "    for idx, (img, _) in enumerate(test_dataloader):\n",
    "        img = img.cuda()\n",
    "        output = model(img)\n",
    "        output1 = output['out'].data.detach().cpu().numpy()\n",
    "        pred.extend(output1)\n",
    "    preds.append(pred)\n",
    "    \n",
    "    model = None\n",
    "    del model, output, pred; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "del test_dataloader\n",
    "\n",
    "## 예측과정에서 메모리 이슈가 발생하는 경우가 많아서 파일을 저장해서 불러오는 식으로 사용\n",
    "np.save('preds(5)', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load('preds(5).npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5폴드 앙상블을 진행 \n",
    "print('fold average')\n",
    "final_answer = np.mean(np.array(preds), axis=0)\n",
    "final_answer = final_answer.argmax(axis=1)\n",
    "\n",
    "# 기존의 저장코드의 속도가 오래 걸려서, 예측된 결과 하나씩 바로 저장 \n",
    "import csv\n",
    "print('write output')\n",
    "out_pred = pd.DataFrame()\n",
    "with open('/home/workspace/user-workspace/prediction/prediction(tree50_deep).csv','w') as f:\n",
    "    writer = csv.writer(f, delimiter = ',')\n",
    "    writer.writerow([c for c in range(512)])\n",
    "    for pred in tqdm(final_answer):\n",
    "        pred = pd.DataFrame(pred)\n",
    "        pred += 1\n",
    "        for idx in range(512):\n",
    "            writer.writerow(pred.loc[idx])\n",
    "    del pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 제출 \n",
    "import os\n",
    "from nipa.taskSubmit import nipa_submit\n",
    "\n",
    "team_id = \"1192\"\n",
    "task_no= \"142\"\n",
    "prediction_path = '/home/workspace/user-workspace/prediction/prediction(tree50_deep).csv'\n",
    "\n",
    "# 파일 존재 여부 확인\n",
    "print(\"is file: \", os.path.isfile(prediction_path))\n",
    "\n",
    "# 제출 성공\n",
    "nipa_submit(team_id=team_id,\n",
    "task_no=task_no,\n",
    "result=prediction_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA 적용시 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_tta(x):\n",
    "    preds = torch.rot90(model(torch.rot90(x, 1, [2, 3]))['out'], -1, [2, 3])\n",
    "    preds += torch.rot90(model(torch.rot90(x, 2, [2, 3]))['out'], -2, [2, 3])\n",
    "    preds += torch.rot90(model(torch.rot90(x, 3, [2, 3]))['out'], -3, [2, 3])\n",
    "    preds += torch.flip(model(torch.flip(x, [2]))['out'], [2])\n",
    "    preds += torch.flip(model(torch.flip(x, [3]))['out'], [3])\n",
    "    preds += model(x)['out']\n",
    "    preds /= 6\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "mode = \"test\"\n",
    "root = '/home/workspace/data/.train/.task142/'\n",
    "\n",
    "batch = 1\n",
    "\n",
    "preds = []\n",
    "for i in range(5):\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False, progress=True, num_classes=9).cuda()\n",
    "    model = load_model('deep_resnet101_pixel75_{}.pth'.format(i), model)\n",
    "    \n",
    "    model.eval()\n",
    "    # get data loader\n",
    "    test_dataloader = data_loader(root=DATASET_PATH, phase=mode, batch_size=batch)\n",
    "    pred = []\n",
    "    for idx, (img, _) in enumerate(test_dataloader):\n",
    "        img = img.cuda()\n",
    "        output = torch_tta(img)\n",
    "        output1 = output.data.detach().cpu().numpy()\n",
    "        pred.extend(output1)\n",
    "    preds.append(pred)\n",
    "    \n",
    "    model = None\n",
    "    del model, output, pred; gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "del test_dataloader\n",
    "np.save('preds(tta)', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean을 취하고 argmax를 하는게 잘못된 것일까?\n",
    "## argmax 이후에 median을 취하는게 나을 수도 있음 \n",
    "print('fold average')\n",
    "final_answer = np.mean(np.array(preds), axis=0)\n",
    "final_answer = final_answer.argmax(axis=1)\n",
    "\n",
    "import csv\n",
    "print('write output')\n",
    "out_pred = pd.DataFrame()\n",
    "with open('/home/workspace/user-workspace/prediction/prediction(tta).csv','w') as f:\n",
    "    writer = csv.writer(f, delimiter = ',')\n",
    "    writer.writerow([c for c in range(512)])\n",
    "    for pred in tqdm(final_answer):\n",
    "        pred = pd.DataFrame(pred)\n",
    "        pred += 1\n",
    "        for idx in range(512):\n",
    "            writer.writerow(pred.loc[idx])\n",
    "    del pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nipa.taskSubmit import nipa_submit\n",
    "\n",
    "team_id = \"1192\"\n",
    "task_no= \"142\"\n",
    "prediction_path = '/home/workspace/user-workspace/prediction/prediction(tta).csv'\n",
    "\n",
    "# 파일 존재 여부 확인\n",
    "print(\"is file: \", os.path.isfile(prediction_path))\n",
    "\n",
    "# 제출 성공\n",
    "nipa_submit(team_id=team_id,\n",
    "task_no=task_no,\n",
    "result=prediction_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
