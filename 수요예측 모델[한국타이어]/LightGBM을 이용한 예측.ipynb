{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/workspace/data/.train/.task152/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = pd.read_csv(path + '6_brand_sales.csv')\n",
    "weather = pd.read_csv(path + '7_weather.csv')\n",
    "\n",
    "brand['sales'] = brand['sales'].apply(lambda x: x.replace('-', '0'))\n",
    "brand['sales'] = brand['sales'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정하는 함수\n",
    "import random\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + '1_daily_sales.csv')\n",
    "stock = pd.read_csv(path + '2_stock.csv') # 2016년도부터 정보가 있음 \n",
    "gdp = pd.read_csv(path + '4_gdp.csv')  # 분기로 값이 나와있어서 월로 변경 필요 \n",
    "car_registration = pd.read_csv(path + '5_car_registration.csv') \n",
    "exchange_rate = pd.read_csv(path + '8_exchange_rate.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby('YYYYMMDD')['Qty'].agg({'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install workalendar\n",
    "from workalendar.asia import SouthKorea # 한국의 공휴일, version : 1.1.1\n",
    "import datetime \n",
    "\n",
    "train['Time'] = pd.to_datetime(train['YYYYMMDD'].astype(str).apply(lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:]))\n",
    "\n",
    "'''\n",
    "from datetime import timedelta, datetime\n",
    "datelist = pd.date_range('2015-01-01', periods=365*5-91).tolist() # pd.date_range(start=\"2015-01-01\",end=\"2020-12-31\")\n",
    "\n",
    "datelist_ = []\n",
    "for i in datelist:\n",
    "    datelist_.append(i.strftime(\"%Y-%m-%d\"))\n",
    "datedf = pd.DataFrame(datelist_)\n",
    "datedf.columns = ['Time']\n",
    "datedf['Time'] = pd.to_datetime(datedf['Time'])\n",
    "\n",
    "datedf = datedf.merge(train, how='left', on='Time')\n",
    "# datedf['sum'] = datedf['sum'].fillna(0)\n",
    "datedf['YYYYMMDD'] = 10000 * datedf['Time'].dt.year + 100 * datedf['Time'].dt.month + 1 * datedf['Time'].dt.day\n",
    "train = datedf.copy()\n",
    "'''\n",
    "\n",
    "train['year'] = train['Time'].dt.year\n",
    "train['month'] = train['Time'].dt.month\n",
    "train['quarter'] = train['Time'].dt.quarter\n",
    "train['week'] = train['Time'].dt.week\n",
    "train['weekday'] = train['Time'].dt.weekday\n",
    "train['day'] = train['Time'].dt.day\n",
    "train['Date'] = train['year'].astype(str) + 'Q' + train['quarter'].astype(str)\n",
    "train['YYMM'] = train['YYYYMMDD'].astype(str).apply(lambda x: x[2:6])\n",
    "\n",
    "# holidays = pd.concat([pd.Series(np.array(SouthKorea().holidays(2019))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2018))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2017))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2016))[:, 0]), pd.Series(np.array(SouthKorea().holidays(2015))[:, 0])]).reset_index(drop=True)\n",
    "# train['holiday'] = train['Time'].dt.date.isin(holidays).astype(int)\n",
    "train['weekend'] = train['weekday'].map({0:0, 1:0, 2:0, 3:0, 4:0, 5:1, 6:1})\n",
    "# train['is_holiday'] = (train['weekend'] + train['holiday']).map({0:0, 1:1, 2:1})\n",
    "train['date'] = pd.to_datetime(train['Time'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neujahr = pd.DataFrame({\n",
    "  'holiday': 'neujahr',\n",
    "  'ds': pd.to_datetime(['2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01'])\n",
    "})\n",
    "\n",
    "karfreitag = pd.DataFrame({\n",
    "  'holiday': 'karfreitag',\n",
    "  'ds': pd.to_datetime(['2015-04-03', '2015-04-06', '2016-03-25', '2016-03-28', '2017-04-14', '2017-04-17', \n",
    "                       '2018-03-30', '2018-04-02', '2019-04-19', '2019-04-22'])\n",
    "})\n",
    "\n",
    "tagderarbeit = pd.DataFrame({\n",
    "  'holiday': 'tagderarbeit',\n",
    "  'ds': pd.to_datetime(['2015-05-01', '2016-05-01', '2017-05-01', '2018-05-01', '2019-05-01'])\n",
    "})\n",
    "\n",
    "ChristiHimmelfahrt = pd.DataFrame({\n",
    "  'holiday': 'ChristiHimmelfahrt',\n",
    "  'ds': pd.to_datetime(['2015-05-14', '2016-05-05', '2017-05-25', '2018-05-10', '2019-05-30'])\n",
    "})\n",
    "\n",
    "pfingetmontag = pd.DataFrame({\n",
    "  'holiday': 'pfingetmontag',\n",
    "  'ds': pd.to_datetime(['2015-05-25', '2016-05-16', '2017-06-05', '2018-05-21', '2019-06-10'])\n",
    "})\n",
    "\n",
    "tagderdeuscheneinheit = pd.DataFrame({\n",
    "  'holiday': 'tagderdeuscheneinheit',\n",
    "  'ds': pd.to_datetime(['2015-10-03', '2016-10-03', '2017-10-03', '2018-10-03', '2019-10-03'])\n",
    "})\n",
    "\n",
    "chrismas = pd.DataFrame({\n",
    "  'holiday': 'chrismas',\n",
    "  'ds': pd.to_datetime(['2015-12-25', '2016-12-25', '2017-12-25', '2018-12-25', '2019-12-25', \n",
    "                       '2015-12-26', '2016-12-26', '2017-12-26', '2018-12-26', '2019-12-26'])\n",
    "})\n",
    "\n",
    "holidays = pd.concat([neujahr, karfreitag, tagderarbeit, ChristiHimmelfahrt, pfingetmontag, \n",
    "                      tagderdeuscheneinheit, chrismas\n",
    "                     ], axis=0)\n",
    "\n",
    "train['holiday'] = train['Time'].dt.date.isin(holidays).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.groupby(['yymm']).agg({'mean'}).reset_index()\n",
    "\n",
    "del weather['region']\n",
    "weather.columns = ['YYMM', 'MAX_TEMP', 'MIN_TEMP', 'AVG_TEMP', 'SNOW_FALL_DAYS', 'SNOW_FALL_CM']\n",
    "\n",
    "stock = stock[stock['YYMM'] > 1512].reset_index(drop=True)\n",
    "stock = stock.astype(np.int)\n",
    "\n",
    "stock['YYMM'] = stock['YYMM'].astype(str)\n",
    "stock.columns =['YYMM', 'WT', 'ASWT', 'ST']\n",
    "train = train.merge(stock, how='left', on='YYMM')\n",
    "\n",
    "gdp.columns = ['Date', 'QGDP', 'QGDPG', 'QGDPAG']\n",
    "train = train.merge(gdp, how='left', on='Date')\n",
    "\n",
    "car_registration.columns = ['YYMM', 'cvm', 'pvm']\n",
    "car_registration['YYMM'] = car_registration['YYMM'].astype(str)\n",
    "train = train.merge(car_registration, how='left', on='YYMM')\n",
    "\n",
    "exchange_rate.columns = ['YYYYMMDD', 'meme']\n",
    "train = train.merge(exchange_rate, how='left', on='YYYYMMDD')\n",
    "\n",
    "weather['YYMM'] = weather['YYMM'].astype(str)\n",
    "train = train.merge(weather, how='left', on='YYMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits= 5\n",
    "seed = 8343\n",
    "n_seeds = 1\n",
    "\n",
    "NUM_BOOST_ROUND = 2000\n",
    "lgbm_param = {'objective':'Regression',\n",
    "              'metric':'RMSE',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'random_state':seed,\n",
    "              'learning_rate':0.01,\n",
    "              'subsample':0.8,\n",
    "              'tree_learner': 'serial',\n",
    "              'max_depth' : 16, \n",
    "#               'reg_lambda':5,\n",
    "#               'reg_alpha': 7,\n",
    "                'num_leaves': 127,   \n",
    "              'verbosity' : -1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import pickle \n",
    "\n",
    "preds = []\n",
    "valid_preds = []\n",
    "feature_importances_full = pd.DataFrame()\n",
    "for te in tqdm(range(1, 32)):\n",
    "    traindata = train.copy()\n",
    "    \n",
    "    #######################\n",
    "    ### 공통 변수 생성 ###\n",
    "    traindata['YYMM'] = traindata['YYMM'].astype(int)\n",
    "    for i in range(1, 2):\n",
    "        traindata['next_yyyymm'] = traindata['YYMM'] + 100\n",
    "        temp_dict = traindata.groupby('next_yyyymm')['sum'].mean()\n",
    "        traindata['before_yyyymm_{}'.format(i)] = traindata['YYYYMMDD'].map(temp_dict)    ### 작년 같은 주, 달의 평균 \n",
    "    \n",
    "    ### 최근 12개월의 평균 ### \n",
    "    for i in range(1, 13):\n",
    "        traindata['next_month'] = traindata['month'] + i\n",
    "        temp_dict = traindata.groupby('next_month')['sum'].mean()\n",
    "        traindata['before_month_{}'.format(i)] = traindata['week'].map(temp_dict)\n",
    "    \n",
    "    #######################\n",
    "    ### 개별 변수 생성 ###\n",
    "    ### 최근 30일간의 거래 ### \n",
    "    for i in range(te, 31):\n",
    "        traindata['sum_shift{}'.format(i)] = traindata['sum'].shift(i)\n",
    "        \n",
    "    ### 최근 8주간의 거래 ###\n",
    "    for i in range((te//5)+1, 9):\n",
    "        traindata['next_week'] = traindata['week'] + i\n",
    "        temp_dict = traindata.groupby('next_week')['sum'].mean()\n",
    "        traindata['before_week_{}'.format(i)] = traindata['week'].map(temp_dict)    \n",
    "        \n",
    "    # Target 을 Shift 해주는 작업 진행 \n",
    "    features = [c for c in traindata.columns if c not in ['', 'next_month', 'next_week', 'date', 'YYYYMMDD', 'sum', 'Time', 'hour', 'YYMM', 'Date', 'tomorrow']]\n",
    "\n",
    "    traindata['target'] = traindata['sum'].shift(-te)\n",
    "    traindata = traindata[traindata['target'].notnull()].reset_index(drop=True)\n",
    "    traindata = traindata[traindata['year'] > 2015].reset_index(drop=True)\n",
    "    \n",
    "    # 변수중요도 기록했다가 하위 n개 삭제해야함 \n",
    "    testdata = traindata.loc[traindata.shape[0] - te]\n",
    "    X_test = pd.DataFrame(testdata[features]).T.reset_index(drop=True).astype(float)\n",
    "                                                   \n",
    "    traindata = traindata.loc[0:traindata.shape[0] - te - 1]\n",
    "    \n",
    "    X_train = traindata[features].astype(float)\n",
    "    y_train = traindata['target']    \n",
    "    \n",
    "    # Train Set / Valid Set 생성 \n",
    "    test_pred = 0\n",
    "    valid_pred = np.zeros(len(X_train))\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = features\n",
    "    \n",
    "    # Group Kfold Year로 변경 \n",
    "    # kfolds = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    kfolds = GroupKFold(n_splits=4)\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kfolds.split(X_train, y_train, groups=X_train['year'])):\n",
    "        \n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        dtrain = lgbm.Dataset( trn_x, trn_y )\n",
    "        dvalid = lgbm.Dataset( val_x, val_y ,reference=dtrain)\n",
    "\n",
    "        model = lgbm.train(lgbm_param , dtrain, NUM_BOOST_ROUND, valid_sets=(dtrain, dvalid), valid_names=('train','valid'), \n",
    "                            verbose_eval = False, early_stopping_rounds=50)\n",
    "        \n",
    "        with open('lightgbm_10month_{}day_{}fold.pkl'.format(te, n_fold), 'wb') as f:\n",
    "            pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)    \n",
    "        \n",
    "        valid_pred[val_idx] = model.predict(X_train.iloc[val_idx])\n",
    "        test_pred  += model.predict(X_test) / 5\n",
    "        \n",
    "                \n",
    "    feature_importances['importance'] = model.feature_importance()\n",
    "    valid_preds.append(mean_squared_error(y_train, valid_pred) ** 0.5)\n",
    "    feature_importances_full = pd.concat([feature_importances_full, feature_importances], axis=0)\n",
    "    preds.append(test_pred)\n",
    "print(np.mean(valid_preds), np.median(valid_preds), np.std(valid_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_imp = feature_importances_full.groupby(['feature'])['importance'].agg({'mean'}).reset_index()\n",
    "fe_imp.sort_values(by='mean', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "datelist = pd.date_range(start=\"2019-10-01\",end=\"2019-10-31\")\n",
    "\n",
    "datelist_ = []\n",
    "for i in datelist:\n",
    "    datelist_.append(i.strftime(\"%Y-%m-%d\"))\n",
    "datedf = pd.DataFrame(datelist_)\n",
    "datedf.columns = ['Time']\n",
    "datedf['Time'] = pd.to_datetime(datedf['Time'])\n",
    "\n",
    "datedf['YYYYMMDD'] = 10000 * datedf['Time'].dt.year + 100 * datedf['Time'].dt.month + 1 * datedf['Time'].dt.day\n",
    "del datedf['Time']\n",
    "\n",
    "datedf['Qty'] = preds[0:31]\n",
    "datedf['Qty'] = datedf['Qty'].apply(lambda x: x[0])\n",
    "\n",
    "sub = pd.read_csv('/home/workspace/data/baseline/task152/prediction/prediction.csv')\n",
    "sub['Time'] = pd.to_datetime(sub['YYYYMMDD'].astype(str).apply(lambda x: x[0:4] + '-' + x[4:6] + '-' + x[6:]))\n",
    "\n",
    "submission = pd.merge(sub, datedf, how='left', on ='YYYYMMDD')\n",
    "submission = submission[submission['Qty_y'].notnull()].reset_index(drop=True)\n",
    "submission = submission[['YYYYMMDD', 'Qty_y']].rename(columns={'Qty_y':'Qty'})\n",
    "submission.to_csv(\"/home/workspace/user-workspace/prediction/prediction(LGB).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# prediction.to_csv(\"/home/workspace/user-workspace/prediction/prediction(Prophet1).csv\", index=False)\n",
    "from nipa.taskSubmit import nipa_submit\n",
    "\n",
    "team_id = \"1192\"\n",
    "task_no= \"152\"\n",
    "\n",
    "prediction_path = \"/home/workspace/user-workspace/prediction/prediction(LGB).csv\"\n",
    "# 파일 존재 여부 확인\n",
    "print(\"is file: \", os.path.isfile(prediction_path))\n",
    "\n",
    "# 제출 성공\n",
    "nipa_submit(team_id=team_id,\n",
    "task_no=task_no,\n",
    "result=prediction_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
